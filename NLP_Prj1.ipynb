{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "93e39ca9-c5ff-4bff-87fe-8cd8ebc898a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install nltk\n",
    "#!pip install --upgrade nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "90b0f412-d192-4161-90f9-5100149f6623",
   "metadata": {},
   "outputs": [],
   "source": [
    "paragraph = \"\"\" A major drawback of statistical methods is that they require elaborate feature engineering. Since 2015,[20] the field has thus largely abandoned statistical methods and shifted to neural networks for machine learning. Popular techniques include the use of word embeddings to capture semantic properties of words, and an increase in end-to-end learning of a higher-level task (e.g., question answering) instead of relying on a pipeline of separate intermediate tasks (e.g., part-of-speech tagging and dependency parsing). In some areas, this shift has entailed substantial changes in how NLP systems are designed, such that deep neural network-based approaches may be viewed as a new paradigm distinct from statistical natural language processing. For instance, the term neural machine translation (NMT) emphasizes the fact that deep learning-based approaches to machine translation directly learn sequence-to-sequence transformations, obviating the need for intermediate steps such as word alignment and language modeling that was used in statistical machine translation (SMT).\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2f20e50a-5d50-476c-9cb4-1b7e09905687",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' A major drawback of statistical methods is that they require elaborate feature engineering. Since 2015,[20] the field has thus largely abandoned statistical methods and shifted to neural networks for machine learning. Popular techniques include the use of word embeddings to capture semantic properties of words, and an increase in end-to-end learning of a higher-level task (e.g., question answering) instead of relying on a pipeline of separate intermediate tasks (e.g., part-of-speech tagging and dependency parsing). In some areas, this shift has entailed substantial changes in how NLP systems are designed, such that deep neural network-based approaches may be viewed as a new paradigm distinct from statistical natural language processing. For instance, the term neural machine translation (NMT) emphasizes the fact that deep learning-based approaches to machine translation directly learn sequence-to-sequence transformations, obviating the need for intermediate steps such as word alignment and language modeling that was used in statistical machine translation (SMT).'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paragraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "089df3d3-9683-423a-81e9-6cbfaf6cdb3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bc9c5415-e6e0-44b1-a974-05a72a87d685",
   "metadata": {},
   "outputs": [],
   "source": [
    "#nltk.download('punkt') \n",
    "#nltk.download('wordnet') \n",
    "#nltk.download('stopwords') #change the server and download\n",
    "sentences = nltk.sent_tokenize(paragraph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e8426be2-3102-493e-b046-39a88e7e620c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[' A major drawback of statistical methods is that they require elaborate feature engineering.',\n",
       " 'Since 2015,[20] the field has thus largely abandoned statistical methods and shifted to neural networks for machine learning.',\n",
       " 'Popular techniques include the use of word embeddings to capture semantic properties of words, and an increase in end-to-end learning of a higher-level task (e.g., question answering) instead of relying on a pipeline of separate intermediate tasks (e.g., part-of-speech tagging and dependency parsing).',\n",
       " 'In some areas, this shift has entailed substantial changes in how NLP systems are designed, such that deep neural network-based approaches may be viewed as a new paradigm distinct from statistical natural language processing.',\n",
       " 'For instance, the term neural machine translation (NMT) emphasizes the fact that deep learning-based approaches to machine translation directly learn sequence-to-sequence transformations, obviating the need for intermediate steps such as word alignment and language modeling that was used in statistical machine translation (SMT).']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "477dfaf6-4a73-4a31-8622-bf0df95fe882",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'histori'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stemmer = PorterStemmer()\n",
    "stemmer.stem('history')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "411a8974-6f6e-4315-a065-259aad0a5c57",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'go'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "lemmatizer.lemmatize('goes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4497e9f9-feee-4dfa-a2d9-2c7d01ab7a50",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "corpus = []\n",
    "for i in range(len(sentences)):\n",
    "    clean_sentence = re.sub('[^a-zA-Z]',' ',sentences[i]) #replacing all the special char with blank space(other than alphabets(a-z, A-Z)\n",
    "    clean_sentence.lower() #changing to lower case\n",
    "    corpus.append(clean_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c16dc524-2132-4dc5-8884-5e8dfba1a2dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[' A major drawback of statistical methods is that they require elaborate feature engineering ',\n",
       " 'Since           the field has thus largely abandoned statistical methods and shifted to neural networks for machine learning ',\n",
       " 'Popular techniques include the use of word embeddings to capture semantic properties of words  and an increase in end to end learning of a higher level task  e g   question answering  instead of relying on a pipeline of separate intermediate tasks  e g   part of speech tagging and dependency parsing  ',\n",
       " 'In some areas  this shift has entailed substantial changes in how NLP systems are designed  such that deep neural network based approaches may be viewed as a new paradigm distinct from statistical natural language processing ',\n",
       " 'For instance  the term neural machine translation  NMT  emphasizes the fact that deep learning based approaches to machine translation directly learn sequence to sequence transformations  obviating the need for intermediate steps such as word alignment and language modeling that was used in statistical machine translation  SMT  ']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37f4dc7c-9ede-4b8b-8401-3d9f850709a8",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a\n",
      "major\n",
      "drawback\n",
      "statist\n",
      "method\n",
      "requir\n",
      "elabor\n",
      "featur\n",
      "engin\n",
      "sinc\n",
      "field\n",
      "thu\n",
      "larg\n",
      "abandon\n",
      "statist\n",
      "method\n",
      "shift\n",
      "neural\n",
      "network\n",
      "machin\n",
      "learn\n",
      "popular\n",
      "techniqu\n",
      "includ\n",
      "use\n",
      "word\n",
      "embed\n",
      "captur\n",
      "semant\n",
      "properti\n",
      "word\n",
      "increas\n",
      "end\n",
      "end\n",
      "learn\n",
      "higher\n",
      "level\n",
      "task\n",
      "e\n",
      "g\n",
      "question\n",
      "answer\n",
      "instead\n",
      "reli\n",
      "pipelin\n",
      "separ\n",
      "intermedi\n",
      "task\n",
      "e\n",
      "g\n",
      "part\n",
      "speech\n",
      "tag\n",
      "depend\n",
      "pars\n",
      "in\n",
      "area\n",
      "shift\n",
      "entail\n",
      "substanti\n",
      "chang\n",
      "nlp\n",
      "system\n",
      "design\n",
      "deep\n",
      "neural\n",
      "network\n",
      "base\n",
      "approach\n",
      "may\n",
      "view\n",
      "new\n",
      "paradigm\n",
      "distinct\n",
      "statist\n",
      "natur\n",
      "languag\n",
      "process\n",
      "for\n",
      "instanc\n",
      "term\n",
      "neural\n",
      "machin\n",
      "translat\n",
      "nmt\n",
      "emphas\n",
      "fact\n",
      "deep\n",
      "learn\n",
      "base\n",
      "approach\n",
      "machin\n",
      "translat\n",
      "directli\n",
      "learn\n",
      "sequenc\n",
      "sequenc\n",
      "transform\n",
      "obviat\n",
      "need\n",
      "intermedi\n",
      "step\n",
      "word\n",
      "align\n",
      "languag\n",
      "model\n",
      "use\n",
      "statist\n",
      "machin\n",
      "translat\n",
      "smt\n"
     ]
    }
   ],
   "source": [
    "## Stemming\n",
    "for i in corpus:\n",
    "    words = nltk.word_tokenize(i)\n",
    "    for word in words:\n",
    "        if word not in set(stopwords.words('english')):\n",
    "            print(stemmer.stem(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7159f02-2044-4abd-a40d-2308168c400d",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i',\n",
       " 'me',\n",
       " 'my',\n",
       " 'myself',\n",
       " 'we',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'you',\n",
       " \"you're\",\n",
       " \"you've\",\n",
       " \"you'll\",\n",
       " \"you'd\",\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves',\n",
       " 'he',\n",
       " 'him',\n",
       " 'his',\n",
       " 'himself',\n",
       " 'she',\n",
       " \"she's\",\n",
       " 'her',\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'it',\n",
       " \"it's\",\n",
       " 'its',\n",
       " 'itself',\n",
       " 'they',\n",
       " 'them',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'themselves',\n",
       " 'what',\n",
       " 'which',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'this',\n",
       " 'that',\n",
       " \"that'll\",\n",
       " 'these',\n",
       " 'those',\n",
       " 'am',\n",
       " 'is',\n",
       " 'are',\n",
       " 'was',\n",
       " 'were',\n",
       " 'be',\n",
       " 'been',\n",
       " 'being',\n",
       " 'have',\n",
       " 'has',\n",
       " 'had',\n",
       " 'having',\n",
       " 'do',\n",
       " 'does',\n",
       " 'did',\n",
       " 'doing',\n",
       " 'a',\n",
       " 'an',\n",
       " 'the',\n",
       " 'and',\n",
       " 'but',\n",
       " 'if',\n",
       " 'or',\n",
       " 'because',\n",
       " 'as',\n",
       " 'until',\n",
       " 'while',\n",
       " 'of',\n",
       " 'at',\n",
       " 'by',\n",
       " 'for',\n",
       " 'with',\n",
       " 'about',\n",
       " 'against',\n",
       " 'between',\n",
       " 'into',\n",
       " 'through',\n",
       " 'during',\n",
       " 'before',\n",
       " 'after',\n",
       " 'above',\n",
       " 'below',\n",
       " 'to',\n",
       " 'from',\n",
       " 'up',\n",
       " 'down',\n",
       " 'in',\n",
       " 'out',\n",
       " 'on',\n",
       " 'off',\n",
       " 'over',\n",
       " 'under',\n",
       " 'again',\n",
       " 'further',\n",
       " 'then',\n",
       " 'once',\n",
       " 'here',\n",
       " 'there',\n",
       " 'when',\n",
       " 'where',\n",
       " 'why',\n",
       " 'how',\n",
       " 'all',\n",
       " 'any',\n",
       " 'both',\n",
       " 'each',\n",
       " 'few',\n",
       " 'more',\n",
       " 'most',\n",
       " 'other',\n",
       " 'some',\n",
       " 'such',\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'only',\n",
       " 'own',\n",
       " 'same',\n",
       " 'so',\n",
       " 'than',\n",
       " 'too',\n",
       " 'very',\n",
       " 's',\n",
       " 't',\n",
       " 'can',\n",
       " 'will',\n",
       " 'just',\n",
       " 'don',\n",
       " \"don't\",\n",
       " 'should',\n",
       " \"should've\",\n",
       " 'now',\n",
       " 'd',\n",
       " 'll',\n",
       " 'm',\n",
       " 'o',\n",
       " 're',\n",
       " 've',\n",
       " 'y',\n",
       " 'ain',\n",
       " 'aren',\n",
       " \"aren't\",\n",
       " 'couldn',\n",
       " \"couldn't\",\n",
       " 'didn',\n",
       " \"didn't\",\n",
       " 'doesn',\n",
       " \"doesn't\",\n",
       " 'hadn',\n",
       " \"hadn't\",\n",
       " 'hasn',\n",
       " \"hasn't\",\n",
       " 'haven',\n",
       " \"haven't\",\n",
       " 'isn',\n",
       " \"isn't\",\n",
       " 'ma',\n",
       " 'mightn',\n",
       " \"mightn't\",\n",
       " 'mustn',\n",
       " \"mustn't\",\n",
       " 'needn',\n",
       " \"needn't\",\n",
       " 'shan',\n",
       " \"shan't\",\n",
       " 'shouldn',\n",
       " \"shouldn't\",\n",
       " 'wasn',\n",
       " \"wasn't\",\n",
       " 'weren',\n",
       " \"weren't\",\n",
       " 'won',\n",
       " \"won't\",\n",
       " 'wouldn',\n",
       " \"wouldn't\"]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bafcfb3-bb10-441c-a300-929f85f10851",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A\n",
      "major\n",
      "drawback\n",
      "statistical\n",
      "method\n",
      "require\n",
      "elaborate\n",
      "feature\n",
      "engineering\n",
      "Since\n",
      "field\n",
      "thus\n",
      "largely\n",
      "abandoned\n",
      "statistical\n",
      "method\n",
      "shifted\n",
      "neural\n",
      "network\n",
      "machine\n",
      "learning\n",
      "Popular\n",
      "technique\n",
      "include\n",
      "use\n",
      "word\n",
      "embeddings\n",
      "capture\n",
      "semantic\n",
      "property\n",
      "word\n",
      "increase\n",
      "end\n",
      "end\n",
      "learning\n",
      "higher\n",
      "level\n",
      "task\n",
      "e\n",
      "g\n",
      "question\n",
      "answering\n",
      "instead\n",
      "relying\n",
      "pipeline\n",
      "separate\n",
      "intermediate\n",
      "task\n",
      "e\n",
      "g\n",
      "part\n",
      "speech\n",
      "tagging\n",
      "dependency\n",
      "parsing\n",
      "In\n",
      "area\n",
      "shift\n",
      "entailed\n",
      "substantial\n",
      "change\n",
      "NLP\n",
      "system\n",
      "designed\n",
      "deep\n",
      "neural\n",
      "network\n",
      "based\n",
      "approach\n",
      "may\n",
      "viewed\n",
      "new\n",
      "paradigm\n",
      "distinct\n",
      "statistical\n",
      "natural\n",
      "language\n",
      "processing\n",
      "For\n",
      "instance\n",
      "term\n",
      "neural\n",
      "machine\n",
      "translation\n",
      "NMT\n",
      "emphasizes\n",
      "fact\n",
      "deep\n",
      "learning\n",
      "based\n",
      "approach\n",
      "machine\n",
      "translation\n",
      "directly\n",
      "learn\n",
      "sequence\n",
      "sequence\n",
      "transformation\n",
      "obviating\n",
      "need\n",
      "intermediate\n",
      "step\n",
      "word\n",
      "alignment\n",
      "language\n",
      "modeling\n",
      "used\n",
      "statistical\n",
      "machine\n",
      "translation\n",
      "SMT\n"
     ]
    }
   ],
   "source": [
    "## Lemmatization\n",
    "for i in corpus:\n",
    "    words = nltk.word_tokenize(i)\n",
    "    for word in words:\n",
    "        if word not in set(stopwords.words('english')):\n",
    "            print(lemmatizer.lemmatize(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5be6834d-aaf2-4bab-bc1a-214a63f43de9",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Apply stop words and lemmatize\n",
    "corpus2= [ ]\n",
    "for i in range(len(sentences)):\n",
    "    clean_sentence = re.sub('[^a-zA-Z]',' ',sentences[i]) #replacing all the special char with blank space(other than alphabets(a-z, A-Z)\n",
    "    clean_sentence = clean_sentence.lower() #changing to lower case\n",
    "    clean_sentence = clean_sentence.split()\n",
    "    clean_sentence = [lemmatizer.lemmatize(word) for word in clean_sentence if word not in set(stopwords.words('english'))]\n",
    "    clean_sentence = ' '.join(clean_sentence)\n",
    "    corpus2.append(clean_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f71f6a82-d681-4128-80b3-71ede8c7888c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['major drawback statistical method require elaborate feature engineering',\n",
       " 'since field thus largely abandoned statistical method shifted neural network machine learning',\n",
       " 'popular technique include use word embeddings capture semantic property word increase end end learning higher level task e g question answering instead relying pipeline separate intermediate task e g part speech tagging dependency parsing',\n",
       " 'area shift entailed substantial change nlp system designed deep neural network based approach may viewed new paradigm distinct statistical natural language processing',\n",
       " 'instance term neural machine translation nmt emphasizes fact deep learning based approach machine translation directly learn sequence sequence transformation obviating need intermediate step word alignment language modeling used statistical machine translation smt']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "043468bc-5a56-48af-a9ec-8b0f8701e231",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Bag of Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "90ad3b40-9b1d-420b-8afc-dd6508f418ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Bag of words\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "cv = CountVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d890955a-b70f-4fb6-960c-be24e36cbc24",
   "metadata": {},
   "outputs": [],
   "source": [
    "bow = cv.fit_transform(corpus2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "83730edf-aabf-43f2-8026-1be89c53d011",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'major': 35,\n",
       " 'drawback': 13,\n",
       " 'statistical': 65,\n",
       " 'method': 37,\n",
       " 'require': 56,\n",
       " 'elaborate': 14,\n",
       " 'feature': 21,\n",
       " 'engineering': 18,\n",
       " 'since': 62,\n",
       " 'field': 22,\n",
       " 'thus': 73,\n",
       " 'largely': 30,\n",
       " 'abandoned': 0,\n",
       " 'shifted': 61,\n",
       " 'neural': 42,\n",
       " 'network': 41,\n",
       " 'machine': 34,\n",
       " 'learning': 32,\n",
       " 'popular': 51,\n",
       " 'technique': 71,\n",
       " 'include': 24,\n",
       " 'use': 76,\n",
       " 'word': 79,\n",
       " 'embeddings': 15,\n",
       " 'capture': 6,\n",
       " 'semantic': 57,\n",
       " 'property': 53,\n",
       " 'increase': 25,\n",
       " 'end': 17,\n",
       " 'higher': 23,\n",
       " 'level': 33,\n",
       " 'task': 70,\n",
       " 'question': 54,\n",
       " 'answering': 2,\n",
       " 'instead': 27,\n",
       " 'relying': 55,\n",
       " 'pipeline': 50,\n",
       " 'separate': 58,\n",
       " 'intermediate': 28,\n",
       " 'part': 49,\n",
       " 'speech': 64,\n",
       " 'tagging': 69,\n",
       " 'dependency': 9,\n",
       " 'parsing': 48,\n",
       " 'area': 4,\n",
       " 'shift': 60,\n",
       " 'entailed': 19,\n",
       " 'substantial': 67,\n",
       " 'change': 7,\n",
       " 'nlp': 44,\n",
       " 'system': 68,\n",
       " 'designed': 10,\n",
       " 'deep': 8,\n",
       " 'based': 5,\n",
       " 'approach': 3,\n",
       " 'may': 36,\n",
       " 'viewed': 78,\n",
       " 'new': 43,\n",
       " 'paradigm': 47,\n",
       " 'distinct': 12,\n",
       " 'natural': 39,\n",
       " 'language': 29,\n",
       " 'processing': 52,\n",
       " 'instance': 26,\n",
       " 'term': 72,\n",
       " 'translation': 75,\n",
       " 'nmt': 45,\n",
       " 'emphasizes': 16,\n",
       " 'fact': 20,\n",
       " 'directly': 11,\n",
       " 'learn': 31,\n",
       " 'sequence': 59,\n",
       " 'transformation': 74,\n",
       " 'obviating': 46,\n",
       " 'need': 40,\n",
       " 'step': 66,\n",
       " 'alignment': 1,\n",
       " 'modeling': 38,\n",
       " 'used': 77,\n",
       " 'smt': 63}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv.vocabulary_ ## Displays the unique words with index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d81c7ae7-c81d-4474-bd0b-ab21f7d858bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'area shift entailed substantial change nlp system designed deep neural network based approach may viewed new paradigm distinct statistical natural language processing'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus2[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "17175e27-4f91-4733-a9d0-80327e8ce7f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1,\n",
       "        1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1,\n",
       "        0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]], dtype=int64)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bow[3].toarray() #13th index is 1 as word drawback is present and index of drawback is 13"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c38951fe-2579-4789-8869-e18806d0e5f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv1 = CountVectorizer(binary=True) ## Only 0 and 1 will be present even if freq greater than 1\n",
    "bow1 = cv1.fit_transform(corpus2) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5d6c9c3d-8c35-4bdb-8b40-59148b50f5f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], dtype=int64)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bow1[0].toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d0e5afe9-7e9e-4518-a11e-4bec7c45ded2",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv2 = CountVectorizer(binary=True , ngram_range=(3,3)) ## Using Trigrams\n",
    "bow2 = cv2.fit_transform(corpus2) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "61b4aa2a-95b3-4811-bd3f-8995c35ce53a",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'major drawback statistical': 40,\n",
       " 'drawback statistical method': 15,\n",
       " 'statistical method require': 73,\n",
       " 'method require elaborate': 42,\n",
       " 'require elaborate feature': 63,\n",
       " 'elaborate feature engineering': 16,\n",
       " 'since field thus': 70,\n",
       " 'field thus largely': 23,\n",
       " 'thus largely abandoned': 84,\n",
       " 'largely abandoned statistical': 32,\n",
       " 'abandoned statistical method': 0,\n",
       " 'statistical method shifted': 74,\n",
       " 'method shifted neural': 43,\n",
       " 'shifted neural network': 69,\n",
       " 'neural network machine': 51,\n",
       " 'network machine learning': 48,\n",
       " 'popular technique include': 59,\n",
       " 'technique include use': 82,\n",
       " 'include use word': 25,\n",
       " 'use word embeddings': 88,\n",
       " 'word embeddings capture': 92,\n",
       " 'embeddings capture semantic': 17,\n",
       " 'capture semantic property': 8,\n",
       " 'semantic property word': 64,\n",
       " 'property word increase': 60,\n",
       " 'word increase end': 93,\n",
       " 'increase end end': 26,\n",
       " 'end end learning': 19,\n",
       " 'end learning higher': 20,\n",
       " 'learning higher level': 35,\n",
       " 'higher level task': 24,\n",
       " 'level task question': 36,\n",
       " 'task question answering': 81,\n",
       " 'question answering instead': 61,\n",
       " 'answering instead relying': 2,\n",
       " 'instead relying pipeline': 28,\n",
       " 'relying pipeline separate': 62,\n",
       " 'pipeline separate intermediate': 58,\n",
       " 'separate intermediate task': 65,\n",
       " 'intermediate task part': 30,\n",
       " 'task part speech': 80,\n",
       " 'part speech tagging': 57,\n",
       " 'speech tagging dependency': 71,\n",
       " 'tagging dependency parsing': 79,\n",
       " 'area shift entailed': 5,\n",
       " 'shift entailed substantial': 68,\n",
       " 'entailed substantial change': 21,\n",
       " 'substantial change nlp': 77,\n",
       " 'change nlp system': 9,\n",
       " 'nlp system designed': 53,\n",
       " 'system designed deep': 78,\n",
       " 'designed deep neural': 12,\n",
       " 'deep neural network': 11,\n",
       " 'neural network based': 50,\n",
       " 'network based approach': 47,\n",
       " 'based approach may': 7,\n",
       " 'approach may viewed': 4,\n",
       " 'may viewed new': 41,\n",
       " 'viewed new paradigm': 90,\n",
       " 'new paradigm distinct': 52,\n",
       " 'paradigm distinct statistical': 56,\n",
       " 'distinct statistical natural': 14,\n",
       " 'statistical natural language': 75,\n",
       " 'natural language processing': 45,\n",
       " 'instance term neural': 27,\n",
       " 'term neural machine': 83,\n",
       " 'neural machine translation': 49,\n",
       " 'machine translation nmt': 38,\n",
       " 'translation nmt emphasizes': 87,\n",
       " 'nmt emphasizes fact': 54,\n",
       " 'emphasizes fact deep': 18,\n",
       " 'fact deep learning': 22,\n",
       " 'deep learning based': 10,\n",
       " 'learning based approach': 34,\n",
       " 'based approach machine': 6,\n",
       " 'approach machine translation': 3,\n",
       " 'machine translation directly': 37,\n",
       " 'translation directly learn': 86,\n",
       " 'directly learn sequence': 13,\n",
       " 'learn sequence sequence': 33,\n",
       " 'sequence sequence transformation': 66,\n",
       " 'sequence transformation obviating': 67,\n",
       " 'transformation obviating need': 85,\n",
       " 'obviating need intermediate': 55,\n",
       " 'need intermediate step': 46,\n",
       " 'intermediate step word': 29,\n",
       " 'step word alignment': 76,\n",
       " 'word alignment language': 91,\n",
       " 'alignment language modeling': 1,\n",
       " 'language modeling used': 31,\n",
       " 'modeling used statistical': 44,\n",
       " 'used statistical machine': 89,\n",
       " 'statistical machine translation': 72,\n",
       " 'machine translation smt': 39}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " cv2.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "e6dc4a24-5b0b-4c0e-863f-8fad7fe52ce4",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv3 = CountVectorizer(binary=True , ngram_range=(2,3)) ## Using both Bigrams and Trigrams\n",
    "bow3 = cv3.fit_transform(corpus2) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "17d7efd7-d3fc-49d3-b307-a46a9463833a",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'major drawback': 81,\n",
       " 'drawback statistical': 30,\n",
       " 'statistical method': 146,\n",
       " 'method require': 85,\n",
       " 'require elaborate': 126,\n",
       " 'elaborate feature': 32,\n",
       " 'feature engineering': 46,\n",
       " 'major drawback statistical': 82,\n",
       " 'drawback statistical method': 31,\n",
       " 'statistical method require': 147,\n",
       " 'method require elaborate': 86,\n",
       " 'require elaborate feature': 127,\n",
       " 'elaborate feature engineering': 33,\n",
       " 'since field': 140,\n",
       " 'field thus': 47,\n",
       " 'thus largely': 167,\n",
       " 'largely abandoned': 66,\n",
       " 'abandoned statistical': 0,\n",
       " 'method shifted': 87,\n",
       " 'shifted neural': 138,\n",
       " 'neural network': 101,\n",
       " 'network machine': 97,\n",
       " 'machine learning': 76,\n",
       " 'since field thus': 141,\n",
       " 'field thus largely': 48,\n",
       " 'thus largely abandoned': 168,\n",
       " 'largely abandoned statistical': 67,\n",
       " 'abandoned statistical method': 1,\n",
       " 'statistical method shifted': 148,\n",
       " 'method shifted neural': 88,\n",
       " 'shifted neural network': 139,\n",
       " 'neural network machine': 103,\n",
       " 'network machine learning': 98,\n",
       " 'popular technique': 118,\n",
       " 'technique include': 163,\n",
       " 'include use': 51,\n",
       " 'use word': 176,\n",
       " 'word embeddings': 184,\n",
       " 'embeddings capture': 34,\n",
       " 'capture semantic': 15,\n",
       " 'semantic property': 128,\n",
       " 'property word': 120,\n",
       " 'word increase': 186,\n",
       " 'increase end': 53,\n",
       " 'end end': 38,\n",
       " 'end learning': 40,\n",
       " 'learning higher': 72,\n",
       " 'higher level': 49,\n",
       " 'level task': 74,\n",
       " 'task question': 161,\n",
       " 'question answering': 122,\n",
       " 'answering instead': 4,\n",
       " 'instead relying': 57,\n",
       " 'relying pipeline': 124,\n",
       " 'pipeline separate': 116,\n",
       " 'separate intermediate': 130,\n",
       " 'intermediate task': 61,\n",
       " 'task part': 159,\n",
       " 'part speech': 114,\n",
       " 'speech tagging': 142,\n",
       " 'tagging dependency': 157,\n",
       " 'dependency parsing': 23,\n",
       " 'popular technique include': 119,\n",
       " 'technique include use': 164,\n",
       " 'include use word': 52,\n",
       " 'use word embeddings': 177,\n",
       " 'word embeddings capture': 185,\n",
       " 'embeddings capture semantic': 35,\n",
       " 'capture semantic property': 16,\n",
       " 'semantic property word': 129,\n",
       " 'property word increase': 121,\n",
       " 'word increase end': 187,\n",
       " 'increase end end': 54,\n",
       " 'end end learning': 39,\n",
       " 'end learning higher': 41,\n",
       " 'learning higher level': 73,\n",
       " 'higher level task': 50,\n",
       " 'level task question': 75,\n",
       " 'task question answering': 162,\n",
       " 'question answering instead': 123,\n",
       " 'answering instead relying': 5,\n",
       " 'instead relying pipeline': 58,\n",
       " 'relying pipeline separate': 125,\n",
       " 'pipeline separate intermediate': 117,\n",
       " 'separate intermediate task': 131,\n",
       " 'intermediate task part': 62,\n",
       " 'task part speech': 160,\n",
       " 'part speech tagging': 115,\n",
       " 'speech tagging dependency': 143,\n",
       " 'tagging dependency parsing': 158,\n",
       " 'area shift': 10,\n",
       " 'shift entailed': 136,\n",
       " 'entailed substantial': 42,\n",
       " 'substantial change': 153,\n",
       " 'change nlp': 17,\n",
       " 'nlp system': 106,\n",
       " 'system designed': 155,\n",
       " 'designed deep': 24,\n",
       " 'deep neural': 21,\n",
       " 'network based': 95,\n",
       " 'based approach': 12,\n",
       " 'approach may': 8,\n",
       " 'may viewed': 83,\n",
       " 'viewed new': 180,\n",
       " 'new paradigm': 104,\n",
       " 'paradigm distinct': 112,\n",
       " 'distinct statistical': 28,\n",
       " 'statistical natural': 149,\n",
       " 'natural language': 91,\n",
       " 'language processing': 65,\n",
       " 'area shift entailed': 11,\n",
       " 'shift entailed substantial': 137,\n",
       " 'entailed substantial change': 43,\n",
       " 'substantial change nlp': 154,\n",
       " 'change nlp system': 18,\n",
       " 'nlp system designed': 107,\n",
       " 'system designed deep': 156,\n",
       " 'designed deep neural': 25,\n",
       " 'deep neural network': 22,\n",
       " 'neural network based': 102,\n",
       " 'network based approach': 96,\n",
       " 'based approach may': 14,\n",
       " 'approach may viewed': 9,\n",
       " 'may viewed new': 84,\n",
       " 'viewed new paradigm': 181,\n",
       " 'new paradigm distinct': 105,\n",
       " 'paradigm distinct statistical': 113,\n",
       " 'distinct statistical natural': 29,\n",
       " 'statistical natural language': 150,\n",
       " 'natural language processing': 92,\n",
       " 'instance term': 55,\n",
       " 'term neural': 165,\n",
       " 'neural machine': 99,\n",
       " 'machine translation': 77,\n",
       " 'translation nmt': 173,\n",
       " 'nmt emphasizes': 108,\n",
       " 'emphasizes fact': 36,\n",
       " 'fact deep': 44,\n",
       " 'deep learning': 19,\n",
       " 'learning based': 70,\n",
       " 'approach machine': 6,\n",
       " 'translation directly': 171,\n",
       " 'directly learn': 26,\n",
       " 'learn sequence': 68,\n",
       " 'sequence sequence': 132,\n",
       " 'sequence transformation': 134,\n",
       " 'transformation obviating': 169,\n",
       " 'obviating need': 110,\n",
       " 'need intermediate': 93,\n",
       " 'intermediate step': 59,\n",
       " 'step word': 151,\n",
       " 'word alignment': 182,\n",
       " 'alignment language': 2,\n",
       " 'language modeling': 63,\n",
       " 'modeling used': 89,\n",
       " 'used statistical': 178,\n",
       " 'statistical machine': 144,\n",
       " 'translation smt': 175,\n",
       " 'instance term neural': 56,\n",
       " 'term neural machine': 166,\n",
       " 'neural machine translation': 100,\n",
       " 'machine translation nmt': 79,\n",
       " 'translation nmt emphasizes': 174,\n",
       " 'nmt emphasizes fact': 109,\n",
       " 'emphasizes fact deep': 37,\n",
       " 'fact deep learning': 45,\n",
       " 'deep learning based': 20,\n",
       " 'learning based approach': 71,\n",
       " 'based approach machine': 13,\n",
       " 'approach machine translation': 7,\n",
       " 'machine translation directly': 78,\n",
       " 'translation directly learn': 172,\n",
       " 'directly learn sequence': 27,\n",
       " 'learn sequence sequence': 69,\n",
       " 'sequence sequence transformation': 133,\n",
       " 'sequence transformation obviating': 135,\n",
       " 'transformation obviating need': 170,\n",
       " 'obviating need intermediate': 111,\n",
       " 'need intermediate step': 94,\n",
       " 'intermediate step word': 60,\n",
       " 'step word alignment': 152,\n",
       " 'word alignment language': 183,\n",
       " 'alignment language modeling': 3,\n",
       " 'language modeling used': 64,\n",
       " 'modeling used statistical': 90,\n",
       " 'used statistical machine': 179,\n",
       " 'statistical machine translation': 145,\n",
       " 'machine translation smt': 80}"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv3.vocabulary_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "498c7bd4-bf32-4b0b-bf5f-7381cdd7f6ce",
   "metadata": {
    "tags": []
   },
   "source": [
    "## TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "d1735977-a3bb-40f7-969e-fef21d0d4f02",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tfidf = TfidfVectorizer()\n",
    "vector = tfidf.fit_transform(corpus2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "257ed56e-ccc7-43bb-98c9-dfe44656866b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'major drawback statistical method require elaborate feature engineering'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus2[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "d8b9e08f-8ed2-445d-be59-9717476afad1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.37882278, 0.37882278,\n",
       "        0.        , 0.        , 0.        , 0.37882278, 0.        ,\n",
       "        0.        , 0.37882278, 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.37882278, 0.        , 0.30563183, 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.37882278, 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.21342214, 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ]])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vector[0].toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4d1f83a-e9a4-4ee8-a8a3-22ea25875935",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b08ee63-9f5b-49af-a8b6-6efc33ac548c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75ee5e32-f978-449a-a89c-e8a5aeb6db5c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87079812-2698-4ca9-8b7b-8d157d1cde32",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c793807-7334-490c-9a62-f35a2f18d0d7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
